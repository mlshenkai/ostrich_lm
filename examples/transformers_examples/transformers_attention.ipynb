{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c56bc7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58469249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f0746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ModelArgs(BaseModel):\n",
    "    dim: int = 1024\n",
    "    n_heads: int = 8\n",
    "    dropout: float = 0.1\n",
    "    max_seq_len: int = 2048\n",
    "    embed_dim: int = 12345\n",
    "    hidden_dim: int = 512\n",
    "    num_encoder_layers: int = 8\n",
    "    num_decoder_layres: int = 8\n",
    "    theta: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02dca22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi attention\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, is_causal: bool=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.dim % args.n_heads == 0\n",
    "\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "\n",
    "        # 构造 qw, kw, vw, ow 参数矩阵\n",
    "\n",
    "        self.qw = nn.Linear(args.embed_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(args.embed_dim, self.head_dim * self.n_heads, bias=False)\n",
    "        self.vw = nn.Linear(args.embed_dim, self.head_dim * self.n_heads, bias=False)\n",
    "\n",
    "\n",
    "        self.ow = nn.Linear(self.head_dim * self.n_heads, args.embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.is_causal = is_causal\n",
    "        if self.is_causal:\n",
    "            mask = torch.full((args.max_seq_len, args.max_seq_len), fill_value=float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "        bsz, seq_len = q.shape[:2]\n",
    "\n",
    "        xq, xk, xv = self.qw(q), self.kw(k), self.vw(v)\n",
    "\n",
    "        # resize\n",
    "        xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if self.is_causal:\n",
    "            scores = scores + self.mask # noqa  # pyright: ignore[reportOperatorIssue]\n",
    "        \n",
    "        scores = F.softmax(scores, dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, xv) # bsz, seq_len, \n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, self.head_dim * self.n_heads)\n",
    "\n",
    "        output = self.ow(output)\n",
    "        return self.dropout(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73be55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.l2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(F.relu(self.l1(x))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b406f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LayerNormal\n",
    "\n",
    "class LayerNormal(nn.Module):\n",
    "    def __init__(self, features, eps: float=1e-6) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.aw = nn.Parameter(torch.ones(features))\n",
    "        self.bw = nn.Parameter(torch.zeros(features))\n",
    "    \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        avg = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        x = (x - avg) / std\n",
    "\n",
    "        return  self.aw * x + self.bw\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b79aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
    "        self.mlp = MLP(args.dim, args.hidden_dim, args.dropout)\n",
    "        self.layer_normal = LayerNormal(args.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm_x = self.layer_normal(x)\n",
    "        x = x + self.attention(norm_x, norm_x, norm_x)\n",
    "\n",
    "        output = x + self.mlp(self.layer_normal(x))\n",
    "        return output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder_layers = [EncoderLayer(args) for _ in range(args.num_encoder_layers)]\n",
    "        self.norm = LayerNormal(args.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8c60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder decoder layer 包含一个 self_attention 和 cross_attention\n",
    "\n",
    "\"\"\"\n",
    "# Transformer Decoder Layer 结构图（ASCII风格）\n",
    "\n",
    "```\n",
    "        +-----------------------------+\n",
    "        |        Input (from prev)    |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        | Masked Multi-Head Attention |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        |      Add & LayerNorm        |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        | Encoder-Decoder Attention   |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        |      Add & LayerNorm        |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        |   Feed Forward Network (FFN)|\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        |      Add & LayerNorm        |\n",
    "        +-----------------------------+\n",
    "                     |\n",
    "                     v\n",
    "        +-----------------------------+\n",
    "        |         Output              |\n",
    "        +-----------------------------+\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(args, is_causal=False)\n",
    "        self.self_attn_norm = LayerNormal(args.embed_dim)\n",
    "        self.attention = MultiHeadAttention(args, is_causal=True)\n",
    "        self.attn_norm = LayerNormal(args.embed_dim)\n",
    "        \n",
    "        self.mlp = MLP(args.dim, args.hidden_dim, args.dropout)\n",
    "        self.mlp_norm = LayerNormal(args.embed_dim)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        norm_x = self.self_attn_norm(x)\n",
    "        x = x + self.self_attention(norm_x, norm_x, norm_x)\n",
    "        norm_x = self.attn_norm(x)\n",
    "        x = x + self.attention(norm_x, encoder_output, encoder_output)\n",
    "\n",
    "        output = x + self.mlp(self.mlp_norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_layers = [DecoderLayer(args) for _ in range(args.num_decoder_layres)] \n",
    "        self.norm = LayerNormal(args.embed_dim)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output)\n",
    "        \n",
    "        return self.norm(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4a9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "## position embedding\n",
    "\n",
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_sql_length: int, embedding_dim: int, theta: int = 10000) -> None:\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_sql_length, embedding_dim))\n",
    "\n",
    "        position = torch.arange(0, max_sql_length).unsqueeze(1)\n",
    "        div_theta = torch.exp(torch.arange(0, embedding_dim, 2)/embedding_dim * -math.log(theta) )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_theta)\n",
    "        pe[:, 1::2] = torch.cos(position * div_theta)\n",
    "\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].requires_grad_(False)  # pyright: ignore[reportIndexIssue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e1d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37230b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
