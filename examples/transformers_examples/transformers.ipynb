{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf635f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##导入\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd7aefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model args 构建\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ModelArgs(BaseModel):\n",
    "    embedding_dim: int = 1024\n",
    "    n_heads: int = 8\n",
    "    max_seq_len: int = 8192\n",
    "    hidden_size: int = 1024\n",
    "    dropout: float = 0.2\n",
    "    num_encoder_layer: int = 8\n",
    "    num_decoder_layer: int = 8\n",
    "    theta: float = 10000.0\n",
    "    vocab_size: int = 32796\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beed3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi head attention \n",
    "### 支持is_causal参数\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, is_causal: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.embedding_dim % args.n_heads == 0\n",
    "\n",
    "        # 计算每一个head 的维度\n",
    "        self.head_dim = args.embedding_dim // args.n_heads\n",
    "        self.n_heads = args.n_heads\n",
    "\n",
    "        self.qw = nn.Linear(args.embedding_dim, self.head_dim * args.n_heads, bias=False)\n",
    "        self.kw = nn.Linear(args.embedding_dim, self.head_dim * args.n_heads, bias=False)\n",
    "        self.vw = nn.Linear(args.embedding_dim, self.head_dim * args.n_heads, bias=False)\n",
    "\n",
    "        self.ow = nn.Linear(self.head_dim * args.n_heads, args.embedding_dim)\n",
    "        self.is_causal = is_causal\n",
    "        if self.is_causal:\n",
    "            mask = torch.full((args.max_seq_len, args.max_seq_len), fill_value=float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "        bsz, seq_len = q.shape[:2]\n",
    "\n",
    "        xq = self.qw(q)\n",
    "        xk = self.kw(k)\n",
    "        xv = self.vw(v)\n",
    "        xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        xk = xk.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        xv = xv.view(bsz, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if self.is_causal:\n",
    "            scores = scores + self.mask[:seq_len, :seq_len]\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(scores, xv)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, self.n_heads * self.head_dim)\n",
    "\n",
    "        output = self.ow(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506570c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.l2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(F.relu(self.l1(x))))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65101f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LayerNormal\n",
    "\n",
    "\n",
    "class LayerNormal(nn.Module):\n",
    "    def __init__(self, features, eps:float=1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.ones(features))\n",
    "        self.b = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        return self.a * x + self.b\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9867b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(args)\n",
    "        self.mlp = MLP(args.embedding_dim, args.hidden_size, args.dropout)\n",
    "        self.attn_norm = LayerNormal(args.embedding_dim)\n",
    "        self.mlp_norm = LayerNormal(args.embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        norm_x = self.attn_norm(x)\n",
    "        x = x + self.attn(norm_x, norm_x, norm_x)\n",
    "        x = x + self.mlp(self.mlp_norm(x)) \n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = [EncoderLayer(args) for _ in range(args.num_encoder_layer)]\n",
    "        self.layer_norm = LayerNormal(args.embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.layer_norm(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e526b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoder\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(args)\n",
    "        self.self_attn_norm = LayerNormal(args.embedding_dim)\n",
    "\n",
    "        self.attn = MultiHeadAttention(args, is_causal=True)\n",
    "        self.attn_norm = LayerNormal(args.embedding_dim)\n",
    "\n",
    "        self.mlp = MLP(args.embedding_dim, args.hidden_size, args.dropout)\n",
    "        self.mlp_norm = LayerNormal(args.embedding_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor):\n",
    "        norm_x = self.self_attn_norm(x)\n",
    "        x = x + self.self_attn(norm_x, norm_x, norm_x)\n",
    "        norm_x = self.attn_norm(x)\n",
    "        x = x + self.attn(norm_x, encoder_out, encoder_out)\n",
    "\n",
    "        x = self.mlp(self.mlp_norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = [DecoderLayer(args) for _ in range(args.num_decoder_layer)]\n",
    "        self.norm = LayerNormal(args.embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_out)\n",
    "        \n",
    "        return  self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8573b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(self, max_seq_len, embedding_dim,theta: float=10000.0) -> None:\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_seq_len, embedding_dim))\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1) # max_seq_len, 1\n",
    "        div_term = torch.exp(torch.arange(0, max_seq_len, 2) * -(math.log(theta)/ embedding_dim))\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sql_len = x.size(1)\n",
    "\n",
    "        return x + self.pe[:,:sql_len, :].requires_grad_(False)  # pyright: ignore[reportIndexIssue]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0b29407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191785/2514506715.py:3: DeprecationWarning: 'uu' is deprecated and slated for removal in Python 3.13\n",
      "  from uu import decode\n"
     ]
    }
   ],
   "source": [
    "## transformer\n",
    "\n",
    "from uu import decode\n",
    "from torch.nn import L1Loss, parameter\n",
    "\n",
    "\n",
    "class Transformers(nn.Module):\n",
    "    def __init__(self, args: ModelArgs) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte = nn.Embedding(self.args.vocab_size, self.args.embedding_dim),\n",
    "                wpe = PositionEmbeddings(self.args.max_seq_len, self.args.embedding_dim, self.args.theta),\n",
    "                drop = nn.Dropout(self.args.dropout),\n",
    "                encoder = Encoder(self.args),\n",
    "                decoder = Decoder(self.args)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.ln_head = nn.Linear(self.args.embedding_dim, self.args.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_model_weight)\n",
    "\n",
    "        print(f\"total parmas: {self.get_num_params()}\")\n",
    "\n",
    "    def _init_model_weight(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def get_num_params(self, non_embedding: bool = False):\n",
    "        n_params = sum([params.numel() for params in self.parameters()])\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wte.weight.numel()  # pyright: ignore[reportAttributeAccessIssue, reportCallIssue]\n",
    "        return n_params\n",
    "\n",
    "    def forward(self, index: torch.Tensor, target=None):\n",
    "        \n",
    "        tok_embed = self.transformer[\"wte\"](index)\n",
    "        pos_embed = self.transformer[\"wpe\"](tok_embed)\n",
    "        # 添加一个drop\n",
    "        x = self.transformer[\"drop\"](pos_embed)\n",
    "\n",
    "\n",
    "\n",
    "        enc_out = self.transformer[\"encoder\"](x)\n",
    "\n",
    "        dec_out = self.transformer[\"decoder\"](x, enc_out)\n",
    "\n",
    "        if target is not None:\n",
    "            logits = self.ln_head(dec_out)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.ln_head(dec_out[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e559f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
