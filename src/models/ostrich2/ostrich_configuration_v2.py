from typing import Any
from torch import dtype
from torch._C import dtype
from transformers import PretrainedConfig

"""
ğŸ“ æ¨¡å‹å‚æ•°:
â”œâ”€â”€ è¯æ±‡è¡¨å¤§å°: 151,936
â”œâ”€â”€ éšè—å±‚ç»´åº¦: 4,096
â”œâ”€â”€ ä¸­é—´å±‚ç»´åº¦: 22,016
â”œâ”€â”€ å±‚æ•°: 32
â”œâ”€â”€ æ³¨æ„åŠ›å¤´æ•°: 32
â”œâ”€â”€ å¤´ç»´åº¦: 128
â”œâ”€â”€ æœ€å¤§åºåˆ—é•¿åº¦: 32,768
â”œâ”€â”€ RoPE theta: 10,000
â””â”€â”€ RMS Norm epsilon: 1e-6

"""


class OstrichConfigurationV2(PretrainedConfig):
    def __init__(self,  **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = 151936
        self.dim = 4096
        self.hidden_size = 22016
        self.num_layers = 32
        self.num_heads = 128
        self.num_kv_heads = 32
        self.max_seq_len = 32768
        self.rope_theta = 10000.0
        self.rms_norm_eps = 1e-6
        self.attention_dropout=0.0
