{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05cfb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "264742ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834e51e",
   "metadata": {},
   "source": [
    "### 模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ae971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OstrichModelConfig(PretrainedConfig):\n",
    "    model_type = \"Ostrich-Llm\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int = 768, # 模型维度\n",
    "            n_layers: int = 12, # Transformer的层数\n",
    "            n_heads: int = 16, # 注意力机制的头数\n",
    "            n_kv_heads: int = 8, # 键值头的数量\n",
    "            vocab_size: int = 6144, # 词汇表大小\n",
    "            hidden_dim: int = None, # 隐藏层维度\n",
    "            multiple_of: int = 64, \n",
    "            norm_eps: float = 1e-5, # 归一化层的eps\n",
    "            max_seq_len: int = 512, # 最大序列长度\n",
    "            dropout: float = 0.0, # dropout概率\n",
    "            flash_attn: bool = True, # 是否使用Flash Attention\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads \n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178589d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(torch.pow(x, 2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75b7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "## repeat_kv\n",
    "### 因为使用 gropu_attn， q使用 n_heads, kv使用 kv_heads， 所以在计算最终的attn时，需要扩增k，v的维度到与v相同的维度\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): k_states or v_states\n",
    "        n_rep (int): repeat的数量\n",
    "    \"\"\"\n",
    "    bsz, seq_len, kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "\n",
    "    expand_x = x[\n",
    "        :, :, :, None, :\n",
    "    ]  # expand_x shape (bsz, seqlen, kv_heads, 1, head_dim)\n",
    "    expand_x = expand_x.expand(bsz, seq_len, kv_heads, n_rep, head_dim).reshape(\n",
    "        bsz, seq_len, kv_heads * n_rep, head_dim\n",
    "    )\n",
    "    return expand_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "950e28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预计算 分组频率 也就是 e^i* theta = cos(theta) + i * sin(theta) i表示虚数单位\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def precompute_freq_cis(max_seq_length, dim, theta: float=10000.0):\n",
    "    # 维度频率计算\n",
    "    freqs = 1.0 / theta ** (torch.arange(0, dim, 2)[:dim//2] / dim).float()\n",
    "\n",
    "    # 生成 序列index,\n",
    "    t = torch.arange(0, max_seq_length).type_as(freqs)\n",
    "\n",
    "    # 外积计算\n",
    "    freqs = torch.outer(t, freqs) # seq_len, dim // 2\n",
    "\n",
    "    # 计算 大小为1 方向为theta的旋转矩阵 即转为复数域\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rope_embedding(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # bsz, seq_len, dim => bsz , seq_len, dim//2 2\n",
    "    # 按照维度进行22分组，x y\n",
    "    xq_ = xq.reshape(*xq.shape[:-1], -1, 2)\n",
    "    xk_ = xk.reshape(*xk.shape[:-1], -1, 2)\n",
    "\n",
    "    # 转为复数 构建为x+iy\n",
    "    xq_ = torch.view_as_complex(xq_)\n",
    "    xk_ = torch.view_as_complex(xk_)\n",
    "\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2).type_as(xq)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2).type_as(xk)\n",
    "    return xq_out, xk_out\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: OstrichModelConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 判别是否包含 n_kv_heads, 若不包含 则n_kv_heads = n_heads\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "\n",
    "        # 确保 n_heads 是否是 n_kv_heads的整数倍\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "\n",
    "        model_parallel_size = 1\n",
    "\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "\n",
    "        self.n_reps = self.n_local_heads // self.n_local_kv_heads\n",
    "\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        freq_cis = precompute_freq_cis(args.max_seq_len, dim=self.head_dim)\n",
    "        self.register_buffer(\"freq_cis\", freq_cis)\n",
    "        self.dropout_prob = args.dropout\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\")\n",
    "        if self.flash:\n",
    "            # 若不支持，则手动mask\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), fill_value=float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        bsz, seq_len = x.shape[:2]\n",
    "\n",
    "        xq = self.wq(x)\n",
    "        xk = self.wk(x)\n",
    "        xv = self.wv(x)\n",
    "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = repeat_kv(xv, self.n_reps)\n",
    "        xk = repeat_kv(xk, self.n_reps)\n",
    "        # transpose\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "        \n",
    "        xq, xk = apply_rope_embedding(xq, xk, self.freq_cis[:seq_len, :])  # pyright: ignore[reportArgumentType, reportIndexIssue]\n",
    "        # 计算 scores\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.flash:\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout_prob, is_causal=True)\n",
    "        else:\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "            assert hasattr(self, \"mask\")\n",
    "            scores = scores + self.mask[:, :, :seq_len, :seq_len]  # pyright: ignore[reportIndexIssue]\n",
    "            \n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            output = torch.matmul(attn, xv)\n",
    "        \n",
    "        # 恢复维度\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "args = OstrichModelConfig()\n",
    "attention_model = Attention(args)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 1\n",
    "seq_len = 50\n",
    "dim = args.dim\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, dim)  # 随机生成输入张量\n",
    "\n",
    "\n",
    "# 运行Attention模型\n",
    "output = attention_model(x)\n",
    "\n",
    "# attention出来之后的形状 依然是[batch_size, seq_len, dim]\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa994d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3, 4, 5)   # shape = [2, 3, 4, 5]\n",
    "y = torch.flatten(x, -2, -1)  # 把最后两个维度合并\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a79943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_size: int, multiple_of: int, dropout_prob: float) -> None:\n",
    "        super().__init__()\n",
    "        if hidden_size is None:\n",
    "            # 如果hidden_size 不设置的话，我们往往会先将其设置为 dim 的4 倍，然后将至 2/3 倍，也就是 8 * dim // 3 \n",
    "            # 另外 hidden_size 应该为multiple_of 的整数倍\n",
    "            hidden_size = int(4 * dim)\n",
    "            hidden_size = int(2 * hidden_size / 3)\n",
    "            hidden_size = ((hidden_size + multiple_of - 1) // multiple_of) * multiple_of           \n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(dim, hidden_size, bias=False)\n",
    "        self.w3 = nn.Linear(hidden_size, dim, bias=False)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.dropout(self.w3(F.selu(self.w1(x)) + self.w2(x)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13edf753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)\n",
    "# 随机生成数据\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "# 运行MLP模型\n",
    "output = mlp(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "546d2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建Decoder\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, args: OstrichModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(args.dim, args.norm_eps)\n",
    "        self.attn = Attention(args)\n",
    "        \n",
    "        self.ffn_norm = RMSNorm(args.dim, args.norm_eps)\n",
    "        self.feed_forward = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.attn_norm(x))\n",
    "        x = x + self.feed_forward(self.ffn_norm(x)) \n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args: OstrichModelConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = [DecoderLayer(args) for _ in range(args.n_layers)]\n",
    "        self.norm = RMSNorm(args.dim, args.norm_eps)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4d7c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(args)\n",
    "# 随机生成数据\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "# 运行MLP模型\n",
    "output = decoder(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4139ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 至此我们开始组装我们的模型\n",
    "from transformers import PreTrainedModel\n",
    "from typing import Optional\n",
    "class OstrichModel(PreTrainedModel):\n",
    "    config_class = OstrichModelConfig\n",
    "    last_loss: Optional[torch.Tensor]\n",
    "    def __init__(self, config: OstrichModelConfig, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        \n",
    "        self.embed = nn.Embedding(args.vocab_size, args.dim)\n",
    "        \n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b124785f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0597,  1.4129,  0.1038,  ..., -1.7825,  1.9595, -1.5581],\n",
       "        [-1.4532, -0.3955,  0.7904,  ..., -2.2277,  0.6763, -0.2762],\n",
       "        [-0.6718, -0.5793,  0.3264,  ..., -0.6065,  0.4360, -1.7508],\n",
       "        [-1.3422, -1.0430,  1.4487,  ..., -2.1306, -1.6287, -1.1258],\n",
       "        [-0.0550,  1.4950,  0.1686,  ..., -0.8293,  1.2068, -0.6586]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.random\n",
    "a = torch.randn((5, 512))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010e8373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0597, 1.4129, 0.1038,  ..., 0.0000, 1.9595, 0.0000],\n",
       "        [0.0000, 0.0000, 0.7904,  ..., 0.0000, 0.6763, 0.0000],\n",
       "        [0.0000, 0.0000, 0.3264,  ..., 0.0000, 0.4360, 0.0000],\n",
       "        [0.0000, 0.0000, 1.4487,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.4950, 0.1686,  ..., 0.0000, 1.2068, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a<0.1] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca76e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
