"""
Qwen3 模型架构详细图 (ASCII 风格)
基于 transformers 库中的 modular_qwen3.py 实现
"""

qwen3_architecture = """
┌─────────────────────────────────────────────────────────────────────────────┐
│                            Qwen3 Model Architecture                        │
│                         Based on Transformer Decoder                        │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                              Input Token IDs                               │
│                           vocab_size = 151,936                             │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Token Embedding                                  │
│                           hidden_size = 4096                               │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                    32x Qwen3DecoderLayer Stack                             │
│ ┌─────────────────────────────────────────────────────────────────────────┐ │
│ │                      Qwen3DecoderLayer                                 │ │
│ │                                                                         │ │
│ │ ┌─────────────────────────────────────────────────────────────────────┐ │ │
│ │ │                   Qwen3Attention Block                             │ │ │
│ │ │                                                                     │ │ │
│ │ │    Input: [B, L, 4096]                                              │ │ │
│ │ │           │                                                         │ │ │
│ │ │           ├─────── Q_proj ──────┐                                   │ │ │
│ │ │           │        (4096→4096)   │                                   │ │ │
│ │ │           │                      ▼                                   │ │ │
│ │ │           │                  Q_norm (RMS)                           │ │ │
│ │ │           │                      │                                   │ │ │
│ │ │           │                      ▼                                   │ │ │
│ │ │           │              Reshape: [B,32,L,128]                      │ │ │
│ │ │           │                      │                                   │ │ │
│ │ │           ├─────── K_proj ──────┤                                   │ │ │
│ │ │           │        (4096→4096)   │                                   │ │ │
│ │ │           │                      ▼                                   │ │ │
│ │ │           │                  K_norm (RMS)                           │ │ │
│ │ │           │                      │                                   │ │ │
│ │ │           │                      ▼                                   │ │ │
│ │ │           │              Reshape: [B,32,L,128] ──┐                  │ │ │
│ │ │           │                                      │                  │ │ │
│ │ │           └─────── V_proj ──────────────────────┤                  │ │ │
│ │ │                    (4096→4096)                   │                  │ │ │
│ │ │                           │                      │                  │ │ │
│ │ │                           ▼                      ▼                  │ │ │
│ │ │                   Reshape: [B,32,L,128]    RoPE(Q,K)               │ │ │
│ │ │                           │                      │                  │ │ │
│ │ │                           └──────┬───────────────┘                  │ │ │
│ │ │                                  │                                  │ │ │
│ │ │                                  ▼                                  │ │ │
│ │ │                     Multi-Head Attention                            │ │ │
│ │ │                   Attention = softmax(QK^T/√d_k)V                   │ │ │
│ │ │                                  │                                  │ │ │
│ │ │                                  ▼                                  │ │ │
│ │ │                       Sliding Window (层28+)                        │ │ │
│ │ │                         window_size=4096                            │ │ │
│ │ │                                  │                                  │ │ │
│ │ │                                  ▼                                  │ │ │
│ │ │                            O_proj (4096→4096)                       │ │ │
│ │ │                                  │                                  │ │ │
│ │ └─────────────────────────────────┼─────────────────────────────────┘ │ │
│ │                                   │                                   │ │
│ │                                   ▼                                   │ │
│ │ ┌─────────────────────────────────────────────────────────────────────┐ │ │
│ │ │                    Residual + RMS Norm                             │ │ │
│ │ │                  output = norm(input + attention)                   │ │ │
│ │ └─────────────────────────────────┬─────────────────────────────────┘ │ │
│ │                                   │                                   │ │
│ │                                   ▼                                   │ │
│ │ ┌─────────────────────────────────────────────────────────────────────┐ │ │
│ │ │                        Qwen3MLP Block                              │ │ │
│ │ │                                                                     │ │ │
│ │ │    Input: [B, L, 4096]                                              │ │ │
│ │ │           │                                                         │ │ │
│ │ │           ├─────── Gate_proj ────────┐                              │ │ │
│ │ │           │       (4096→22016)       │                              │ │ │
│ │ │           │                          ▼                              │ │ │
│ │ │           │                     SiLU(gate)                          │ │ │
│ │ │           │                          │                              │ │ │
│ │ │           │                          │                              │ │ │
│ │ │           └─────── Up_proj ─────────┼──── ⊗ (element-wise multiply) │ │ │
│ │ │                   (4096→22016)       │                              │ │ │
│ │ │                                      │                              │ │ │
│ │ │                                      ▼                              │ │ │
│ │ │                              Down_proj                              │ │ │
│ │ │                             (22016→4096)                            │ │ │
│ │ │                                      │                              │ │ │
│ │ └─────────────────────────────────────┼─────────────────────────────┘ │ │
│ │                                       │                               │ │
│ │                                       ▼                               │ │
│ │ ┌─────────────────────────────────────────────────────────────────────┐ │ │
│ │ │                    Residual + RMS Norm                             │ │ │
│ │ │                  output = norm(input + mlp)                         │ │ │
│ │ └─────────────────────────────────────────────────────────────────────┘ │ │
│ └─────────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Final RMS Norm                                   │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Language Model Head                                │
│                    Linear: 4096 → vocab_size(151936)                       │
└─────────────────────────────┬───────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Output Logits                                     │
│                      Vocabulary Distribution                                │
└─────────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════
                               关键特性详解
═══════════════════════════════════════════════════════════════════════════════

🔧 Qwen3 独有特性:
├── Q/K Normalization: 在 head_dim 维度上进行 RMS 归一化
├── 混合注意力模式:
│   ├── 前 28 层: 全局注意力 (full_attention)
│   └── 后 4 层: 滑动窗口注意力 (sliding_attention, window=4096)
├── 继承自 Qwen2 架构但有改进
└── 兼容多种注意力后端 (eager, flash_attention, sdpa)

📐 模型参数:
├── 词汇表大小: 151,936
├── 隐藏层维度: 4,096
├── 中间层维度: 22,016
├── 层数: 32
├── 注意力头数: 32
├── 头维度: 128
├── 最大序列长度: 32,768
├── RoPE theta: 10,000
└── RMS Norm epsilon: 1e-6

🧮 关键计算公式:
├── Attention: softmax(QK^T/√d_k)V
├── Q_norm: RMSNorm(Q) = Q / sqrt(mean(Q²) + ε)
├── K_norm: RMSNorm(K) = K / sqrt(mean(K²) + ε)
├── MLP: Down(SiLU(Gate(x)) ⊗ Up(x))
└── RoPE: 旋转位置编码应用于 Q 和 K

⚡ 数据流维度:
├── Input: [batch_size, seq_len] → Embedding → [batch_size, seq_len, 4096]
├── Attention: [B, L, 4096] → [B, 32, L, 128] → [B, L, 4096]
├── MLP: [B, L, 4096] → [B, L, 22016] → [B, L, 4096]
└── Output: [B, L, 4096] → [B, L, 151936] (logits)

🔄 继承关系:
├── Qwen3Config ← PretrainedConfig
├── Qwen3RMSNorm ← Qwen2RMSNorm
├── Qwen3MLP ← GemmaMLP
├── Qwen3Attention ← LlamaAttention (+ Q/K norm)
├── Qwen3DecoderLayer ← Qwen2DecoderLayer
├── Qwen3Model ← Qwen2Model
└── Qwen3ForCausalLM ← Qwen2ForCausalLM
"""

def save_ascii_diagram():
    """保存 ASCII 架构图到文件"""
    output_path = '/Users/shenkai/code/transformers-1/qwen3_architecture_ascii.txt'

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(qwen3_architecture)

    print(f"Qwen3 ASCII 架构图已保存到: {output_path}")
    return output_path

if __name__ == "__main__":
    save_ascii_diagram()
    print("\n" + qwen3_architecture)